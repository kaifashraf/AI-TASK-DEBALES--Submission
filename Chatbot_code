!pip install flask-ngrok
!pip install langchain sentence_transformers faiss-cpu flask flask_restful requests beautifulsoup4 transformers torch
import os
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from flask import Flask, request, jsonify
from flask_restful import Api, Resource
import threading
import time
import requests
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

loader = WebBaseLoader("https://brainlox.com/courses/category/technical")
data = loader.load()


text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(data)


embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(documents, embeddings)


model_name = "gpt2"  # You can change this to a larger model if Colab's GPU allows
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)


text_generation_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.15
)


llm = HuggingFacePipeline(pipeline=text_generation_pipeline)


qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

app = Flask(_name_)
api = Api(app)

class ChatbotConversation(Resource):
    def post(self):
        data = request.get_json()
        query = data.get('query')
        chat_history = data.get('chat_history', [])

        try:
            result = qa_chain({"question": query, "chat_history": chat_history})
            response = {
                "answer": result['answer'],
                "source_documents": [doc.page_content for doc in result['source_documents']]
            }
        except Exception as e:
            response = {"error": str(e)}

        return jsonify(response)

api.add_resource(ChatbotConversation, '/chat')

def run_flask():
    app.run(port=5000)


flask_thread = threading.Thread(target=run_flask)
flask_thread.start()

time.sleep(5)

def ask_chatbot(query, chat_history=[]):
    url = "http://localhost:5000/chat"
    data = {"query": query, "chat_history": chat_history}
    try:
        response = requests.post(url, json=data)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        return {"error": f"Failed to get response: {str(e)}"}
from google.colab import output

def chat():
    chat_history = []
    print("Chatbot is ready. You can now ask questions.")
    print("Type 'exit' to end the conversation.")

    while True:
        query = input("You: ")
        if query.lower() == 'exit':
            break

        response = ask_chatbot(query, chat_history)
        if "error" in response:
            print(f"Error: {response['error']}")
        else:
            print(f"Chatbot: {response['answer']}")
            chat_history.append((query, response['answer']))

    print("Conversation ended.")

from flask_ngrok import run_with_ngrok
run_with_ngrok(app)  # Start ngrok when the app is run

chat()
